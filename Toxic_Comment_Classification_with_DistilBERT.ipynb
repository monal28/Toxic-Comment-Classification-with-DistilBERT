{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Cw3zTrngoBl"
   },
   "source": [
    "# **Section 1 - Setting up the environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "424ba21b"
   },
   "source": [
    "## Mount Google Drive & Set Working Directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN0ZCmMtiU07"
   },
   "source": [
    "We start by mounting Google Drive to access our dataset and navigating to the appropriate folder for training assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhYP4aO87uH3"
   },
   "outputs": [],
   "source": [
    "# After cloning the GitHub repo into Colab\n",
    "%cd /content\n",
    "\n",
    "!git clone https://github.com/monal28/Toxic-Comment-Classification-with-DistilBERT.git\n",
    "\n",
    "%cd Toxic-Comment-Classification-with-DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kc_PkSlHBJgt",
    "outputId": "6425e680-532b-4633-db74-8aabb0996d0c"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#import os\n",
    "\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#folder_path = '/content/drive/MyDrive/NLP3'\n",
    "#os.chdir(folder_path)\n",
    "\n",
    "# List files in the working directory\n",
    "#!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5a4e031"
   },
   "source": [
    "## Check GPU Availability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Hq29Sg0iYC1"
   },
   "source": [
    "We detect the number of available GPUs to adjust our batch size accordingly and accelerate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_oEXq4pJiIz",
    "outputId": "39795988-8fd8-4297-e424-58596934f8a0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Detect number of GPUs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(\"Number of GPUs:\", n_gpu)\n",
    "\n",
    "# Batch size and training config\n",
    "BASE_BATCH_SIZE = 16\n",
    "BATCH_SIZE = BASE_BATCH_SIZE * max(1, n_gpu)\n",
    "EPOCHS = 3\n",
    "MAX_LEN = 192\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tt_VEvicfN5T",
    "outputId": "1e780b92-67ed-4928-b18a-9f0b6c677268"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "638af18e"
   },
   "source": [
    "## Import Essential Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aY-4uen0iQ7A"
   },
   "source": [
    "We import essential libraries for:\n",
    "- Data handling (`pandas`, `numpy`)\n",
    "- Model training (`torch`, `transformers`)\n",
    "- Evaluation (`sklearn`)\n",
    "- Visualization (`matplotlib`, `seaborn`, `plotly`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcKKQuO_xskt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data & Progress\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Transformers and Tokenizers\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lyuNyK5gwAc"
   },
   "source": [
    "# **Section 2 - Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "904d8ee9"
   },
   "source": [
    "## Load Dataset\n",
    "We load the multilingual toxic comment dataset from Jigsaw, including training, validation, and test sets. A smaller subset is used for faster experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7d8Jg1yBLzy"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('jigsaw-toxic-comment-train.csv', nrows=10000)\n",
    "valid = pd.read_csv('validation.csv')\n",
    "test = pd.read_csv('test.csv', nrows=5000)\n",
    "test_labels = pd.read_csv(\"test_labels.csv\", nrows=5000)\n",
    "#sub = pd.read_csv('sample_submission.csv', nrows=5000)\n",
    "\n",
    "#sub_en = pd.read_csv('sample_submission.csv', nrows=5000)\n",
    "test_en = pd.read_csv('jigsaw-toxic-comment-train.csv')\n",
    "test_en = test_en.tail(5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5f7339a"
   },
   "source": [
    "## Data Preprocessing\n",
    "- Tokenize text using the `distilbert-base-multilingual-cased` tokenizer\n",
    "- Pad and truncate to 192 tokens\n",
    "- Format inputs with attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269,
     "referenced_widgets": [
      "2b0b63d856e846178f22df33292d2f7b",
      "39034577875a4ae4ac21d102500c808f",
      "5c0773148c544df09d0d572baf2cbd15",
      "10c11c60656e4225ac351a37ea447790",
      "c852e93a0e6a4adf9c7b12658c0852ed",
      "c3065c1572094f0da80aa3f56854e37c",
      "021c690336a5410e84183f00c33b4d50",
      "51b68534c8d6470fb9e178c1b2711ae4",
      "1b923a9d247e443b963814679c4b17da",
      "37f6046ddf0a44fbab8cf34e1f15b9f0",
      "55895601fc0e4cbbbc49a19a58cfd166",
      "24a93ec6384848d59a2a9c03bec5f744",
      "f9f1d9abfc4b4694a6c753c8c4e90e72",
      "f416cc9df28d45a299231c5cec4b9691",
      "5c7b9a335ea94929a4355f3f1290348d",
      "1ff4414ca94c4194bc48a2d8484fa76e",
      "6a96108269b74ed29aa58f9429f0bb06",
      "0c04c90d0da54bf0a411723f001afe34",
      "a3f488d9118e4538aac5b262926ca446",
      "f223ae6492cb4f91bce01750c699695d",
      "d60da1eec343433faedc95da3f33b238",
      "a1ff0b2de6184755b018e44a62e8f98f",
      "d6f35b02be284023bb47fbce9317b393",
      "38f9ecefeb0d41119486ba00ef9e04e4",
      "3a377323fb0a47529de004b892ae3cb7",
      "cb9dc5741c024be4b72736ed4fb3c47c",
      "72f354dfddf447ef8e7e6e295cbddde2",
      "13d7b0ee9d72486aaf5ee4a1a31bcab8",
      "d263cd8b76a24476a1240c77d16348e1",
      "576e6816b5204d1d9ac7c157b126a2eb",
      "22dd07d811b34010b23a21e373c9cce5",
      "f34a387d364345778e1108449395ffc0",
      "8a69b85510804583b857eb54252fbb90",
      "2c3c0cb39a0e478f8ab066f03a32113c",
      "dd032e8550b349c385f8cd277b514e1c",
      "9e077c53c3224137bcbeefe109800417",
      "084094ccf3f34abbaa1d39f5966487e0",
      "a06962367ea043e1a02c71deaed1cdcc",
      "7b554133a4b1414ba4a99ab1c0911b2e",
      "b9a7ec84b1794c5e9c3a20bd78f1d78b",
      "1c852b3b44e6423e95afbfc46a127dfc",
      "900ea35ec331493abccb608481af4b7a",
      "d8225e17f5b9487fa107d89a42bb83f3",
      "f33d518686f34ffd93cb518c5620cdd0"
     ]
    },
    "id": "kqy04Z46GNKg",
    "outputId": "297f8302-7d32-4abd-fb6f-8bbc750f2d2f"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6DTUdPoGgTa"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def encode_dataset(texts, tokenizer, max_len=512, chunk_size=256):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), chunk_size), desc=\"Tokenizing\"):\n",
    "        text_chunk = texts[i:i + chunk_size].tolist()\n",
    "        encodings = tokenizer(\n",
    "            text_chunk,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encodings['input_ids'])\n",
    "        attention_masks.append(encodings['attention_mask'])\n",
    "\n",
    "    # Concatenate all chunks\n",
    "    all_input_ids = torch.cat(input_ids)\n",
    "    all_attention_masks = torch.cat(attention_masks)\n",
    "    return all_input_ids, all_attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UuVKLFUAHu6F",
    "outputId": "8d41fba8-3758-45b5-fc38-4ec14d8142af"
   },
   "outputs": [],
   "source": [
    "x_train_input_ids, x_train_attention_mask = encode_dataset(train.comment_text.astype(str), tokenizer, max_len=MAX_LEN)\n",
    "x_valid_input_ids, x_valid_attention_mask = encode_dataset(valid.comment_text.astype(str), tokenizer, max_len=MAX_LEN)\n",
    "x_test_input_ids, x_test_attention_mask = encode_dataset(test.content.astype(str), tokenizer, max_len=MAX_LEN)\n",
    "x_test_en_input_ids, x_test_en_attention_mask = encode_dataset(test_en.comment_text.astype(str), tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "y_train = torch.tensor(train.toxic.values, dtype=torch.float)\n",
    "y_valid = torch.tensor(valid.toxic.values, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkU7sY8-gfW6"
   },
   "source": [
    "## Define PyTorch Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ke_xuGigKpxY"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C6Ua6nQgMZq"
   },
   "source": [
    "##  Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzX9brVxLf34"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ToxicDataset(x_train_input_ids, x_train_attention_mask, y_train)\n",
    "valid_dataset = ToxicDataset(x_valid_input_ids, x_valid_attention_mask, y_valid)\n",
    "test_dataset = ToxicDataset(x_test_input_ids, x_test_attention_mask)\n",
    "test_dataset_en = ToxicDataset(x_test_en_input_ids, x_test_en_attention_mask)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "test_loader_en = DataLoader(test_dataset_en, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48Cq4Jf4hDrP"
   },
   "source": [
    "# **Section 3 - Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03e290c0"
   },
   "source": [
    "## Model Definition\n",
    "We use a DistilBERT model architecture with a custom binary classification head:\n",
    "\n",
    "- Pretrained base: `distilbert-base-multilingual-cased`\n",
    "\n",
    "- Final layer: Fully connected (Linear) layer mapping to a single output for binary classification (toxic vs non-toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "69475a20131d4258ab4e5452a71fb834",
      "c2bf7510a86045aeb6bb9feb76fc0dbc",
      "af7b13514b674f259fba22d6ee240b5f",
      "5db1ebfb8bc4429386290a70f42bc27e",
      "9936a92e41274880810cbe68b0a4e306",
      "0c967e095f6347e997dccf1c95a336da",
      "0b6433adbcb04eae8bdbdf90ef736e12",
      "ca00381dc0d041348e32974ca060fd35",
      "4c2bfdfd51904067889b0d89ee21fa8c",
      "fcf8aa3aa0fc48fe8b2f55279a062da6",
      "8d9c403d79704b7eaeacf916009a3a8f"
     ]
    },
    "id": "ivw-4EmGMzUe",
    "outputId": "fe7bd295-8f4f-43f9-8da4-7de43b6a6796"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class ToxicClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-multilingual-cased\"):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # use [CLS] token\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits\n",
    "\n",
    "model = ToxicClassifier().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbJC5F81hTjL"
   },
   "source": [
    "## Training Setup\n",
    "We configure the training loop using:\n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "Loss function: Weighted `BCEWithLogitsLoss` to account for class imbalance\n",
    "\n",
    "Metrics: Accuracy and ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1648149b"
   },
   "source": [
    "### Class Weighting (Imbalance Handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta0kBYxnM0VF"
   },
   "outputs": [],
   "source": [
    "# Count positive (toxic) and negative (non-toxic) samples\n",
    "num_pos = y_train.sum()\n",
    "num_neg = len(y_train) - num_pos\n",
    "\n",
    "# Compute weight for positive class (toxic)\n",
    "pos_weight_value = num_neg / num_pos\n",
    "\n",
    "# Wrap it in a tensor\n",
    "pos_weight = torch.tensor([pos_weight_value], dtype=torch.float).to(device)\n",
    "\n",
    "# Now use it in your loss function\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMNqbb0zkzvU"
   },
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yRsm5jXM8GO"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].unsqueeze(1).to(device)  # shape: (batch, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4CqDHiRk3QF"
   },
   "source": [
    "### Evaluation Function\n",
    "Calculates:\n",
    "\n",
    "- Validation loss\n",
    "\n",
    "- Accuracy\n",
    "\n",
    "- ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYYXX5ofNA8j"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_metrics(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device).float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(outputs)  # Convert logits to probabilities\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # For AUC\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    val_loss = total_loss / len(dataloader)\n",
    "    val_acc = correct / total\n",
    "    val_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    return val_loss, val_acc, val_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tORWlUHWhYIR"
   },
   "source": [
    "### Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84n63esINCzs",
    "outputId": "24c027ad-142d-42e4-ec2b-b299e9887d74"
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_auc = evaluate_metrics(model, valid_loader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCblYQ0kgh3I"
   },
   "source": [
    "# **Section 4 - Evaluation Function**\n",
    "This section presents performance evaluation of the trained model using:\n",
    "\n",
    "- Accuracy\n",
    "\n",
    "- ROC-AUC\n",
    "\n",
    "- Confusion Matrix\n",
    "\n",
    "- Multilingual and English-only test sets\n",
    "\n",
    "- Real-time prediction on personal input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXIQg23YDk7c"
   },
   "source": [
    "## Multilingual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoktLfCVQdZp",
    "outputId": "295d5da7-49d5-4738-e6bf-ef48838f36cf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import torch\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()  # convert logits to probs\n",
    "\n",
    "        all_probs.extend(probs)\n",
    "\n",
    "# Load true labels from test_labels DataFrame\n",
    "true = test_labels['toxic'].values\n",
    "\n",
    "# Convert predictions to NumPy array\n",
    "probs = np.array(all_probs).flatten()\n",
    "pred_labels = (probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "acc = accuracy_score(true, pred_labels)\n",
    "auc = roc_auc_score(true, probs)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuOxM_RAlzH_"
   },
   "source": [
    "### Confusion Matrix (Multilingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "58p1wIdGSrls",
    "outputId": "93ce3fd6-2c77-464a-9ab8-c6001ebbeb9e"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true, pred_labels)\n",
    "\n",
    "# Display it\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-toxic\", \"Toxic\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix Multilingual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXubYpBWDoRd"
   },
   "source": [
    "## English-only Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_ezOcjFQwYL",
    "outputId": "5f4b5702-58da-4f0c-9407-d766f5cd9cf0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "all_probs_en = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_en:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()  # convert logits to probabilities\n",
    "        all_probs_en.extend(probs)\n",
    "\n",
    "# Convert probabilities to binary labels\n",
    "pred_probs_en = np.array(all_probs_en).flatten()\n",
    "pred_labels_en = (pred_probs_en > 0.5).astype(int)\n",
    "\n",
    "# Load true labels from test_en DataFrame\n",
    "true_labels = test_en[\"toxic\"].values\n",
    "\n",
    "# Evaluate\n",
    "acc_en = accuracy_score(true_labels, pred_labels_en)\n",
    "auc_en = roc_auc_score(true_labels, pred_probs_en)\n",
    "\n",
    "print(f\"Accuracy: {acc_en:.4f}\")\n",
    "print(f\"AUC: {auc_en:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lSMQwTQmQZk"
   },
   "source": [
    "### Confusion Matrix (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "LDxKl32lRLSe",
    "outputId": "340462b8-ba97-4615-bb26-152802c5cc5d"
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels_en)\n",
    "\n",
    "# Display it\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-toxic\", \"Toxic\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix EN\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cllpHf9HfN5W"
   },
   "source": [
    "## Personal Input Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdvxIpqwfN5W"
   },
   "outputs": [],
   "source": [
    "def predict_text(text, model, tokenizer, max_len=192):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()[0][0]\n",
    "\n",
    "    # Classification\n",
    "    label = \"Toxic\" if probs > 0.5 else \"Non-toxic\"\n",
    "    return label, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91d31c39"
   },
   "source": [
    "### Example Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ygOw8KxffN5W",
    "outputId": "548320bf-9181-4bb1-e770-fbee189b1a71"
   },
   "outputs": [],
   "source": [
    "text = \"The design is terrible.\"\n",
    "label, probability = predict_text(text, model, tokenizer)\n",
    "print(f\"Label: {label} (Confidence: {probability:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9aGOpqtfN5W"
   },
   "source": [
    "# **Section 5 â€” Save and Load Model**\n",
    "This section outlines how to persist the trained model and tokenizer for later use, enabling:\n",
    "\n",
    "- Efficient reuse without retraining\n",
    "\n",
    "- Easy deployment or sharing\n",
    "\n",
    "- Quick inference and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hiw0O7lsh4z2"
   },
   "source": [
    "## Saving the Model & Tokenizer\n",
    "We save:\n",
    "\n",
    "- The model weights (`.pt` file)\n",
    "\n",
    "- The tokenizer configuration (`./tokenizer/` directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEdV6sWknK0c"
   },
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "torch.save(model.state_dict(), \"toxic_model_v1.pt\")\n",
    "\n",
    "# Save tokenizer config and vocab\n",
    "tokenizer.save_pretrained(\"./tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9crs-c4h78L"
   },
   "source": [
    "## Loading the Model & Tokenizer\n",
    "When reloading for evaluation, deployment, or further training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oy-qdx9OfN5W",
    "outputId": "5b7e7d1f-c486-4068-fad4-135726b0027f"
   },
   "outputs": [],
   "source": [
    "# Re-instantiate model architecture\n",
    "model = ToxicClassifier()\n",
    "\n",
    "# Load saved weights\n",
    "model.load_state_dict(torch.load(\"toxic_model_v1.pt\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dg1msSdlfN5W"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec0a6bc2"
   },
   "source": [
    "# **Section 6 - Summary**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4QQlBJ7nfJ1"
   },
   "source": [
    "This project demonstrates the effectiveness of using DistilBERT for multilingual toxic comment classification, leveraging:\n",
    "\n",
    "- A lightweight yet powerful transformer architecture\n",
    "\n",
    "- Fine-tuning on labeled multilingual data\n",
    "\n",
    "- Evaluation across multilingual and English-only datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arTRtwronv4a"
   },
   "source": [
    "## Key Takeaways\n",
    "- **Strong Performance:** The model achieves high accuracy and ROC-AUC scores, especially when tested on both multilingual and English-specific samples.\n",
    "\n",
    "- **Generalizability:** Despite being trained on a subset, the classifier generalizes well across languages.\n",
    "\n",
    "- **Efficient Inference:** DistilBERT enables fast predictions without major compromises in performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlHAbcionhpb"
   },
   "source": [
    "## Future Work\n",
    "To further enhance results, potential next steps include:\n",
    "\n",
    "- Training with larger or full-scale datasets\n",
    "\n",
    "- Exploring deeper transformer models like XLM-R, BERTweet, or RoBERTa\n",
    "\n",
    "- Applying ensemble methods to combine strengths of multiple classifiers\n",
    "\n",
    "- Integrating contextual filtering or user behavior data for improved moderation accuracy\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
